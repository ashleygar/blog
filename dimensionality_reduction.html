<!DOCTYPE html>
<html lang="en">
<head>
 <title>Dimensionality Reduction</title>
 <!-- Latest compiled and minified CSS -->
 <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous">
 <div class="container">
  <h1><a href=".">blog</a></h1>
 </div>
</head>
<body>
 <div class="container">
<div class="row">
 <div class="col-md-8">
  <h3>Dimensionality Reduction</h3>
  <label>2020-01-26</label>
  <h2>Dimesionality Reduction</h2>
<p>In the current data environment, there is a seemingly infinite amount of data available. It is encumbant on those analyzing data to decide which of the hundreds or thousands of variables are relevant, thereby avoiding a model that is overfitting with a high variance. However, if too many variables are eliminated, the model will be too simple and reflect high bias.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Dimension_levels.svg/800px-Dimension_levels.svg.png" style="height: 175px"></p>
<p>Image source: https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Dimension_levels.svg/800px-Dimension_levels.svg.png</p>
<p>Not only is a high dimensional space difficult to conceptualize or visualize, regions in these spaces typically have vast volumes.</p>
<p>Aurélien Géron summarizes one of the primary issues of high dimensionality in his book, <em>Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow</em>. Imagine 2 points in a unit square. The distance between them is about 0.52. In 3-D, it increases to about 0.66.  In a 1,000,000-dimensional hypercube, it grows to almost 409.  Volumes in high dimensional spaces are huge, resulting in points being spread far apart. This means that not only will training data points inherently have large distances between one another, but that new data points will also be far from training points, making predicitons less accurate.  This is what is commonly framed as the "curse of dimensionality" [1].</p>
<h3>Mathematical Approaches to Dimensionality Reduction</h3>
<p>In dimensionality reduction, there are two mathematical approaches: projection and manifold learning.</p>
<h4>1. Projection</h4>
<p>Projection is when datapoints in a high-dimensional space are projected to a lower-dimensional subspace [1].  The 3-D datapoints are projected perpendicularly to the 2-D plane that best preserves the location of each point [1].  As a result, the relationship between the data point in the high dimensional space to the corresonding data point in the low dimensional space is linear.</p>
<p><img src="images/3d_plot.jpg" style="height: 300px"/></p>
<p>Image source: https://github.com/ageron/handson-ml/blob/master/08_dimensionality_reduction.ipynb</p>
<h4>2. Manifold Learning</h4>
<p>Manifold learning entails modeling datapoints in a high dimensional space that can eventually be spread out to a low dimensional space while still preserving its local shape [1].  The relationship between the data point in the high dimensional space to the corresonding data point in the low dimensional space is nonlinear [2].  Think of it like projecting a globe on to an atlas.  Another example is the Swiss roll below.</p>
<p><img src="images/swissroll.png" style="height: 300px"/></p>
<p><CENTER>
Swiss roll dataset
<CENTER></p>
<p><img src="images/swissroll_flat1.png" style="height: 220px"/>
<p><p/>
Swiss roll flattened without (left) and with (right) manifold learning.
<p><p/></p>
<p>Images source: https://github.com/ageron/handson-ml/blob/master/08_dimensionality_reduction.ipynb</p>
<h3>Methods for Dimensionality Reduction</h3>
<p>The methods to reduce dimensionality that this post will focus on are Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE).  They are projection and manifold learning methods, respectively.</p>
<h4>1. Principal Component Analysis (PCA)</h4>
<p>In PCA, an n number of principal components, or new axes, are created while preserving the most amount of variance [1].  The relationship between the original dataset and the new principal components is linear [3]. The mean square distance is minimized between the original data and the new axis [1]. Therefore, if there are outliers in the data, this may adversely influence the principal component by potentially creating unreliable principal components that are skewed towards the outliers, but not the rest of the datapoints - i.e. the mean square error will be compromised by the outlier data, but not effectively minimized for the rest of the data.</p>
<p>The following example will illustrate the use of PCA on the Iris dataset from sklearn.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="p">)</span>

<span class="c1"># Data source: https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html</span>
</pre></div>


<p>The components attribute will display the principal components matrix.  These are the axes that represent the maximum variance in the data [4].</p>
<div class="highlight"><pre><span></span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span>
</pre></div>


<p>The explained variance ratio sorts the components by the percentage of the variance explained [4].  In this example, the first component contains about 92% of the model's variance.  The second explains about 5% and the third explains about 2%.</p>
<div class="highlight"><pre><span></span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
</pre></div>


<p>If the data were to be visualized, it would appear like the chart below.  The three flowers are well separated.</p>
<p><img src="images/iris_pca.png" style="height: 500px"/></p>
<p>Image source: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html</p>
<p>More information on using PCA with sklearn can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</p>
<h4>2. t-SNE</h4>
<p>t-SNE is a nonlinear dimensionality reduction technique used to reconstruct high dimensional data into a low dimensional space by keeping similar data points close together and dissimilar data points apart from one another in the low dimensional space [1].  It is especially effective at visualization as it captures the structure - both local and global - of the data at many different scales [5]. This contrasts with some methods that tend to crowd points together [5].</p>
<p>Each time the t-SNE equation is run, it generates new results since each run generates probabilities, not distances [7].  The solution ulimately selected will be that with the lowest Kullback-Leiber divergence - a cost method that rewards the preservation of the local structure [5] . In other words, when transforming the data from a high to low dimensionality space, mapping similar and dissimilar points close to and far from one another, respectively, is optimal.</p>
<p>While preparing to run t-SNE, it is recommended to first use another dimenionality reduction technique first that will bring the total number of dimensions to a more reasonable number first (e.g. 50) [6].  This will aid in reducing the time to run t-SNE given that it is a computationally heavy techique, though it may not lead to improved results - this depends on the data [1]. </p>
<p>Running t-SNE in sklearn is similar to other algorithms:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">()</span>
<span class="c1"># tsne.fit(data)</span>
</pre></div>


<p>Parameters to be aware of while using t-SNE include: n_parameters and perplexity.</p>
<ul>
<li>
<p>n_parameters sets the dimension of the space and 2 is the default [6].  While this can be increased, it should be done with caution since one of the main objectives in t-SNE is to reduce the crowding problem, which arises when too many datapoints are forced into a small space [7]. Thus, when embedding into a somewhat higher dimension space, the overcrowding problem does not arise and t-SNE might not be the optimal technique. </p>
</li>
<li>
<p>Perplexity is similar to the measure used in the nearest neighbors k in other manifold learning algorithms [7]. In general, a larger/denser dataset will require more perplexity and typical ranges are from 5 to 50 [7].</p>
</li>
</ul>
<p>As mentioned above, the Kullback-Leiber divergence is the attribute used to measure the performance of the alogorithm.  In sklearn, this attribute is the kl_divergence_ [6].</p>
<p>More information on using t-SNE in sklearn can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html</p>
<p>Below is the t-SNE performance on the MNIST dataset.  At a glance, one can see that t-SNE performs well in separtating the digits.</p>
<p><img src="https://lvdmaaten.github.io/tsne/examples/mnist_tsne.jpg" style="height: 400px;"/></p>
<p>Image source: https://lvdmaaten.github.io/tsne/examples/mnist_tsne.jpg</p>
<p>Some applications of t-SNE include cancer search, music analysis, computer security resarch, and bioinformatics [8].</p>
<p><strong>Sources:</strong></p>
<p>[1] Géron, Aurélien. <em>Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow.</em>  2nd ed., O'Reilly, 2019.</p>
<p>[2] https://scikit-learn.org/stable/modules/manifold.html</p>
<p>[3] https://en.wikipedia.org/wiki/Principal_component_analysis</p>
<p>[4] https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</p>
<p>[5] van der Maaten, Laurens and Hinton, Geoffrey. <em>Visualizing ata using t-SNE.</em> Journal of Machine Learning Research, November 2008, p.2579-2605.</p>
<div class="highlight"><pre><span></span><span class="n">Paper</span> <span class="n">can</span> <span class="n">also</span> <span class="n">be</span> <span class="k">found</span> <span class="k">at</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">lvdmaaten</span><span class="p">.</span><span class="n">github</span><span class="p">.</span><span class="n">io</span><span class="o">/</span><span class="n">publications</span><span class="o">/</span><span class="n">papers</span><span class="o">/</span><span class="n">JMLR_2008</span><span class="p">.</span><span class="n">pdf</span>
</pre></div>


<p>[6] https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html</p>
<p>[7] https://lvdmaaten.github.io/tsne/</p>
<p>[8] https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding</p>
 </div>
</div>
 </div>
</body>
</html>